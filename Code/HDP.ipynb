{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import HdpModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [line.rstrip('\\n') for line in open('./grolier15276_words.txt')]\n",
    "idx2word = {(i+1): words[i] for i in range(len(words))}\n",
    "\n",
    "corpus = []\n",
    "with open(\"./grolier15276.csv\") as file:\n",
    "    for line in file:\n",
    "        doc = []\n",
    "        line = line.split(',')[1:]\n",
    "        if len(line) > 5:  #keep only articles with more than 5 words\n",
    "            for i in range(len(line)//2):\n",
    "                idx, count = int(line[2*i]), int(line[2*i+1])\n",
    "                doc = doc + [idx2word[idx]]*count \n",
    "            corpus.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boilerplate code for gensim\n",
    "common_dictionary = Dictionary(corpus)\n",
    "common_corpus = [common_dictionary.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes ~10 minutes to run\n",
    "hdp = HdpModel(common_corpus, common_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*war + 0.004*government + 0.004*united + 0.003*world + 0.003*american + 0.003*century + 0.003*political'),\n",
       " (1,\n",
       "  '0.006*art + 0.005*century + 0.003*music + 0.003*american + 0.003*life + 0.003*world + 0.003*style'),\n",
       " (2,\n",
       "  '0.004*called + 0.003*water + 0.003*system + 0.003*form + 0.003*energy + 0.002*time + 0.002*earth'),\n",
       " (3,\n",
       "  '0.007*km + 0.007*mi + 0.006*population + 0.006*sq + 0.006*south + 0.005*north + 0.005*deg'),\n",
       " (4,\n",
       "  '0.012*species + 0.009*family + 0.005*cm + 0.005*found + 0.005*plants + 0.005*ft + 0.004*north'),\n",
       " (5,\n",
       "  '0.028*city + 0.009*mi + 0.009*km + 0.009*center + 0.009*century + 0.008*population + 0.008*river'),\n",
       " (6,\n",
       "  '0.007*american + 0.007*university + 0.005*library + 0.005*volumes + 0.005*enrollment + 0.005*won + 0.004*college'),\n",
       " (7,\n",
       "  '0.008*king + 0.005*war + 0.004*john + 0.004*son + 0.004*roman + 0.003*emperor + 0.003*france'),\n",
       " (8,\n",
       "  '0.003*american + 0.003*life + 0.002*include + 0.002*plays + 0.002*eng + 0.002*disease + 0.002*blood'),\n",
       " (9,\n",
       "  '0.002*alcoholism + 0.002*agricultural + 0.001*social + 0.001*food + 0.001*system + 0.001*development + 0.001*american'),\n",
       " (10,\n",
       "  '0.004*accounting + 0.002*anatomy + 0.002*financial + 0.001*air + 0.001*analog + 0.001*public + 0.001*blood'),\n",
       " (11,\n",
       "  '0.002*ft + 0.002*airships + 0.002*world + 0.001*war + 0.001*airship + 0.001*gas + 0.001*air'),\n",
       " (12,\n",
       "  '0.003*albania + 0.002*country + 0.002*alps + 0.001*km + 0.001*government + 0.001*mi + 0.001*world'),\n",
       " (13,\n",
       "  '0.006*languages + 0.003*spoken + 0.002*adams + 0.002*language + 0.002*people + 0.002*american + 0.002*africa'),\n",
       " (14,\n",
       "  '0.002*age + 0.002*bronze + 0.002*field + 0.002*crete + 0.002*mainland + 0.001*aegean + 0.001*mycenaean'),\n",
       " (15,\n",
       "  '0.008*air + 0.004*force + 0.003*forces + 0.002*war + 0.001*world + 0.001*united + 0.001*foster'),\n",
       " (16,\n",
       "  '0.003*afghanistan + 0.002*soviet + 0.001*american + 0.001*country + 0.001*century + 0.001*major + 0.001*united'),\n",
       " (17,\n",
       "  '0.003*plane + 0.003*coordinate + 0.002*system + 0.002*space + 0.002*coordinates + 0.002*aerospace + 0.002*acid'),\n",
       " (18,\n",
       "  '0.002*art + 0.001*academies + 0.001*alcohols + 0.001*academy + 0.001*alberti + 0.001*allegory + 0.001*american'),\n",
       " (19,\n",
       "  '0.003*airports + 0.002*airport + 0.001*service + 0.001*aircraft + 0.001*air + 0.001*world + 0.001*traffic')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get word distribution (top 7 probabilities) for each topic\n",
    "hdp.show_topics(num_words=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.9292840800658455), (5, 0.06726839933817662)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get topic distribution for the 4th article in the corpus\n",
    "hdp[common_corpus[3]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
